from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
from datetime import datetime


class F1DataService:
    pass


class F1ScraperService(F1DataService):
    def __init__(self):
        self.driver = None
        self.timeout = 10
        self.base_url = "https://www.formula1.com"

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞"""
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option('excludeSwitches', ['enable-automation'])

        self.driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=options
        )
        return self

    async def stop(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        if self.driver:
            self.driver.quit()

    async def _get_page(self, url):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ HTML"""
        if not self.driver:
            await self.start()

        self.driver.get(url)
        time.sleep(3)  # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
        return BeautifulSoup(self.driver.page_source, 'html.parser')

    async def get_race_calendar(self, season=None):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–ª–µ–Ω–¥–∞—Ä—è –≥–æ–Ω–æ–∫"""
        try:
            url = f"{self.base_url}/en/racing/{season}.html" if season else f"{self.base_url}/en/racing.html"
            soup = await self._get_page(url)

            events = soup.find_all('div', class_='event-item')
            if not events:
                return ["No races found"]

            calendar = []
            for event in events:
                name = event.find('p', class_='card-title').text.strip()
                date = event.find('p', class_='date').text.strip()
                location = event.find('p', class_='event-place').text.strip()
                calendar.append(f"üèÅ {name}\nüìÖ {date}\nüìç {location}")

            return calendar

        except Exception as e:
            return [f"‚ö†Ô∏è Parsing error: {str(e)}"]

    async def get_last_results(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            soup = await self._get_page(f"{self.base_url}/en/results.html")

            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥–æ–Ω–∫—É
            last_race = soup.find('a', class_='resultsarchive-filter-item-link', href=True)
            if not last_race:
                return {'error': 'No race data found'}

            race_url = f"{self.base_url}{last_race['href']}"
            race_soup = await self._get_page(race_url)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            race_name = race_soup.find('h1', class_='ResultsArchiveTitle').text.strip()
            date = race_soup.find('span', class_='full-date').text.strip()
            results = []

            for row in race_soup.select('table.resultsarchive-table tr')[1:6]:  # –¢–æ–ø-5
                pos = row.find('td', class_='dark').text.strip()
                driver = row.find('span', class_='hide-for-tablet').text.strip()
                team = row.find('td', class_='semi-bold').text.strip()
                results.append(f"{pos}. {driver} ({team})")

            return {
                'race_name': race_name,
                'date': date,
                'results': results,
                'circuit': race_soup.find('span', class_='circuit-info').text.strip() if race_soup.find('span',
                                                                                                        class_='circuit-info') else 'Unknown'
            }

        except Exception as e:
            return {'error': f"‚ö†Ô∏è Parsing error: {str(e)}"}

    async def get_drivers(self, season=None):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –≥–æ–Ω—â–∏–∫–æ–≤"""
        try:
            url = f"{self.base_url}/en/drivers.html"
            soup = await self._get_page(url)

            drivers = []
            for driver in soup.select('div.col-12.col-md-6.col-xl-4'):
                name = driver.find('span', class_='d-block f1--xxs').text.strip()
                team = driver.find('p', class_='f1--s').text.strip()
                number = driver.find('span', class_='f1-bold--xs').text.strip()
                drivers.append(f"üèé {number} {name}\nüèÅ {team}")

            return drivers[:20]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-20 –≥–æ–Ω—â–∏–∫–æ–≤

        except Exception as e:
            return [f"‚ö†Ô∏è Parsing error: {str(e)}"]

    async def get_teams(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –∫–æ–º–∞–Ω–¥"""
        try:
            url = f"{self.base_url}/en/teams.html"
            soup = await self._get_page(url)

            teams = []
            for team in soup.select('div.col-12.col-md-6.col-xl-3'):
                name = team.find('span', class_='d-block f1--xxs').text.strip()
                drivers = team.find_all('p', class_='f1--s')
                driver_list = " & ".join([d.text.strip() for d in drivers])
                teams.append(f"üèÅ {name}\nüèé {driver_list}")

            return teams

        except Exception as e:
            return [f"‚ö†Ô∏è Parsing error: {str(e)}"]

    async def get_standings(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç—É—Ä–Ω–∏—Ä–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã"""
        try:
            soup = await self._get_page(f"{self.base_url}/en/standings.html")

            # –ü–∞—Ä—Å–∏–Ω–≥ –≥–æ–Ω—â–∏–∫–æ–≤
            drivers = []
            for row in soup.select('table.f1-standing-table--driver tr')[1:11]:  # –¢–æ–ø-10
                pos = row.find('td', class_='f1-bold--xs').text.strip()
                name = row.find('span', class_='d-block f1-bold--s').text.strip()
                points = row.find('td', class_='f1-bold--end').text.strip()
                drivers.append(f"{pos}. {name} - {points} pts")

            # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–∞–Ω–¥
            teams = []
            for row in soup.select('table.f1-standing-table--team tr')[1:6]:  # –¢–æ–ø-5
                pos = row.find('td', class_='f1-bold--xs').text.strip()
                name = row.find('span', class_='d-block f1-bold--s').text.strip()
                points = row.find('td', class_='f1-bold--end').text.strip()
                teams.append(f"{pos}. {name} - {points} pts")

            return {
                'drivers': drivers,
                'teams': teams
            }

        except Exception as e:
            return {'error': f"‚ö†Ô∏è Parsing error: {str(e)}"}


async def create_f1_service():
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    service = F1ScraperService()
    await service.start()
    return service